# Research Agent Configuration
# Copy to config.local.yaml and modify for your setup

# ============================================
# Model Configuration
# ============================================
model:
  # Provider options:
  # - "auto" (recommended) - Auto-detect best available provider
  # - "openai" - OpenAI API (requires OPENAI_API_KEY)
  # - "groq" - Groq API with free tier (requires GROQ_API_KEY)
  # - "openrouter" - OpenRouter with free models (requires OPENROUTER_API_KEY)
  # - "ollama" - Local Ollama server
  # - "huggingface" - Local HuggingFace models (requires GPU)
  # - "openai_compatible" - Custom OpenAI-compatible endpoint
  #
  # With "auto", the agent will check for API keys in order:
  # 1. OPENAI_API_KEY -> use OpenAI
  # 2. GROQ_API_KEY -> use Groq (free tier!)
  # 3. OPENROUTER_API_KEY -> use OpenRouter (has free models)
  # 4. Ollama server -> use Ollama if running
  # 5. Fall back to HuggingFace (requires GPU)

  provider: "auto"

  # HuggingFace model (for provider: "huggingface")
  name: "Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4"

  # Ollama settings (if provider: "ollama")
  ollama_base_url: "http://localhost:11434"
  ollama_model: "qwen2.5:32b-instruct-q4_K_M"

  # Generation settings
  max_new_tokens: 2048
  temperature: 0.7
  top_p: 0.9
  repetition_penalty: 1.1

  # OpenAI settings (provider: "openai")
  openai:
    base_url: "https://api.openai.com/v1"
    api_key_env: "OPENAI_API_KEY"
    models:
      - "gpt-4o-mini"
      - "gpt-4.1-mini"
      - "gpt-4.1"

  # OpenAI-compatible settings (provider: "openai_compatible")
  # Use this for local servers like llama-server, vllm, etc.
  openai_compatible:
    base_url: "http://localhost:8082/v1"
    api_key_env: "OPENAI_COMPAT_API_KEY"
    models:
      - "devstral-small-24b"

# ============================================
# Embedding Model
# ============================================
embedding:
  name: "BAAI/bge-large-en-v1.5"
  device: "cuda"  # "cuda" or "cpu"
  dimension: 1024
  
  # Optional reranker for better retrieval
  reranker:
    enabled: true
    model: "BAAI/bge-reranker-base"

# ============================================
# Vector Store
# ============================================
vector_store:
  type: "chromadb"  # "chromadb" or "qdrant"
  persist_directory: "./data/chroma_db"
  
  # Qdrant settings (if type: "qdrant")
  # qdrant_url: "http://localhost:6333"
  # qdrant_api_key: null

# ============================================
# Retrieval Settings
# ============================================
retrieval:
  top_k: 5
  similarity_threshold: 0.7
  
  # Chunking for document ingestion
  chunk_size: 512
  chunk_overlap: 50

# ============================================
# Academic Search APIs
# ============================================
search:
  semantic_scholar:
    enabled: true
    # Free tier: 100 requests per 5 minutes
    rate_limit_requests: 100
    rate_limit_window: 300  # seconds
    
  openalex:
    enabled: true
    # Very generous limits, no key needed
    email: null  # Optional: set for polite pool
    
  unpaywall:
    enabled: true
    email: null  # Required for API access
    
  crossref:
    enabled: true
    email: null  # Optional: for polite pool
    
  # Web search for grey literature
  web_search:
    enabled: true
    provider: "tavily"  # "tavily" or "serper"
    # API key loaded from environment: TAVILY_API_KEY or SERPER_API_KEY

# ============================================
# Knowledge Base Ingestion
# ============================================
ingestion:
  # Require user approval before adding sources
  auto_ingest: false
  
  # If auto_ingest: true, minimum relevance score to auto-add
  auto_threshold: 0.85
  
  # Quality filters
  require_abstract: true
  min_year: null  # e.g., 2015
  
  # Citation chain exploration
  follow_citations: true
  max_citation_depth: 1

# ============================================
# Data Analysis
# ============================================
analysis:
  # Default visualization settings
  figure_dpi: 150
  default_style: "seaborn-v0_8-whitegrid"
  
  # Export formats
  export_formats:
    - "png"
    - "csv"

# ============================================
# User Interface
# ============================================
ui:
  port: 7860
  host: "127.0.0.1"
  share: false  # Set true to create public link
  
  # Theme
  theme: "soft"  # "soft", "default", "glass", etc.

# ============================================
# Logging
# ============================================
logging:
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR
  file: "./logs/research_agent.log"
  console: true

# ============================================
# Researcher Lookup
# ============================================
researcher_lookup:
  input_file: "./data/researchers.txt"
  output_dir: "./data/researchers"
  sources:
    openalex: true
    semantic_scholar: true
    web_search: true
  web_search_provider: "duckduckgo"  # free, no key required
  request_delay: 0.5  # seconds between API requests

# ============================================
# Paths
# ============================================
paths:
  data_dir: "./data"
  cache_dir: "./cache"
  exports_dir: "./exports"
  logs_dir: "./logs"
