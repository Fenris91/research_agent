# Research Agent Configuration
# Copy to config.local.yaml and modify for your setup

# ============================================
# Model Configuration
# ============================================
model:
  # Provider options:
  # - "auto" (recommended) - Auto-detect best available provider
  # - "openai" - OpenAI API (requires OPENAI_API_KEY)
  # - "anthropic" - Anthropic Claude (requires ANTHROPIC_API_KEY)
  # - "groq" - Groq API with free tier (requires GROQ_API_KEY)
  # - "openrouter" - OpenRouter with free models (requires OPENROUTER_API_KEY)
  # - "ollama" - Local Ollama server
  # - "huggingface" - Local HuggingFace models (requires GPU)
  # - "openai_compatible" - Custom OpenAI-compatible endpoint
  # - "none" - Retrieval-only mode (no LLM, still searches and formats results)
  #
  # With "auto", the agent will check for API keys in order:
  # 1. OPENAI_API_KEY -> use OpenAI
  # 2. ANTHROPIC_API_KEY -> use Anthropic (Claude)
  # 3. GROQ_API_KEY -> use Groq (free tier!)
  # 4. OPENROUTER_API_KEY -> use OpenRouter (has free models)
  # 5. Ollama server -> use Ollama if running
  # 6. GPU available -> use HuggingFace
  # 7. Nothing -> retrieval-only mode (still works!)

  provider: "groq"

  # HuggingFace model (for provider: "huggingface")
  name: "Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4"

  # Ollama settings (if provider: "ollama")
  ollama_base_url: "http://localhost:11434"
  ollama_model: "qwen2.5:32b-instruct-q4_K_M"

  # Generation settings
  max_new_tokens: 2048
  temperature: 0.7
  top_p: 0.9
  repetition_penalty: 1.1

  # Multi-model pipeline: assign different models per task type.
  # If unset, every task uses the default model above.
  # Task types: classify, extract_keywords, synthesize
  # pipeline:
  #   classify: "llama-3.1-8b-instant"          # fast/cheap for classification
  #   extract_keywords: "llama-3.1-8b-instant"   # fast for keyword extraction
  #   synthesize: "llama-3.3-70b-versatile"      # capable for synthesis

  # OpenAI settings (provider: "openai")
  openai:
    base_url: "https://api.openai.com/v1"
    api_key_env: "OPENAI_API_KEY"
    models:
      - "gpt-5.2"
      - "gpt-4o-mini"
      - "gpt-4.1-mini"
      - "gpt-4.1"

  # OpenAI-compatible settings (provider: "openai_compatible")
  # Use this for local servers like llama-server, vllm, etc.
  openai_compatible:
    base_url: "http://localhost:8082/v1"
    api_key_env: "OPENAI_COMPAT_API_KEY"
    models:
      - "devstral-small-24b"

# ============================================
# Embedding Model
# ============================================
embedding:
  # NOTE: Must match the dimension of existing embeddings in the vector store
  # bge-base-en-v1.5 = 768 dims, bge-large-en-v1.5 = 1024 dims
  name: "BAAI/bge-base-en-v1.5"
  device: "cuda"  # "cuda" or "cpu"
  dimension: 768
  
  # Optional reranker for better retrieval
  reranker:
    enabled: true
    model: "BAAI/bge-reranker-base"

# ============================================
# Vector Store
# ============================================
vector_store:
  type: "chromadb"  # "chromadb" or "qdrant"
  persist_directory: "./data/chroma_db"
  
  # Qdrant settings (if type: "qdrant")
  # qdrant_url: "http://localhost:6333"
  # qdrant_api_key: null

# ============================================
# Retrieval Settings
# ============================================
retrieval:
  top_k: 5
  similarity_threshold: 0.7
  
  # Chunking for document ingestion
  chunk_size: 512
  chunk_overlap: 50

# ============================================
# Academic Search APIs
# ============================================
search:
  semantic_scholar:
    enabled: true
    # Free tier: 100 requests per 5 minutes
    rate_limit_requests: 100
    rate_limit_window: 300  # seconds
    
  openalex:
    enabled: true
    # Very generous limits, no key needed
    email: null  # Optional: set for polite pool
    
  unpaywall:
    enabled: true
    email: null  # Required for API access
    
  crossref:
    enabled: true
    email: null  # Optional: for polite pool
    
  # Web search for grey literature
  web_search:
    enabled: true
    provider: "duckduckgo"  # "duckduckgo" (free, no key), "tavily", or "serper"
    # API key loaded from environment: TAVILY_API_KEY or SERPER_API_KEY

# ============================================
# Knowledge Base Ingestion
# ============================================
ingestion:
  # Require user approval before adding sources
  auto_ingest: false
  
  # If auto_ingest: true, minimum relevance score to auto-add
  auto_threshold: 0.85
  
  # Quality filters
  require_abstract: true
  min_year: null  # e.g., 2015
  
  # Citation chain exploration
  follow_citations: true
  max_citation_depth: 1

# ============================================
# Data Analysis
# ============================================
analysis:
  # Default visualization settings
  figure_dpi: 150
  default_style: "seaborn-v0_8-whitegrid"
  
  # Export formats
  export_formats:
    - "png"
    - "csv"

# ============================================
# User Interface
# ============================================
ui:
  port: 7860
  host: "127.0.0.1"
  share: false  # Set true to create public link
  
  # Theme
  theme: "soft"  # "soft", "default", "glass", etc.

# ============================================
# Knowledge Explorer
# ============================================
explorer:
  enabled: true
  mode: "public"          # "public" or "local"
  email: null             # OpenAlex/Unpaywall polite pool
  openalex_key: null
  semantic_scholar_key: null
  core_key: null
  cache_ttl: 86400        # 24h -- paper metadata rarely changes
  cache_dir: "./cache/explorer"

# ============================================
# Logging
# ============================================
logging:
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR
  file: "./logs/research_agent.log"
  console: true

# ============================================
# Researcher Lookup
# ============================================
researcher_lookup:
  input_file: "./data/researchers.txt"
  output_dir: "./data/researchers"
  sources:
    openalex: true
    semantic_scholar: true
    web_search: true
  web_search_provider: "duckduckgo"  # free, no key required
  request_delay: 0.5  # seconds between API requests

# ============================================
# Paths
# ============================================
paths:
  data_dir: "./data"
  cache_dir: "./cache"
  exports_dir: "./exports"
  logs_dir: "./logs"
